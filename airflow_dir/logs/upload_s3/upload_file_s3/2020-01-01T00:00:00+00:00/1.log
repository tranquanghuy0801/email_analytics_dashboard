[2020-03-26 21:11:33,380] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 21:11:33,408] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 21:11:33,408] {taskinstance.py:866} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 21:11:33,408] {taskinstance.py:867} INFO - Starting attempt 1 of 1
[2020-03-26 21:11:33,408] {taskinstance.py:868} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 21:11:33,433] {taskinstance.py:887} INFO - Executing <Task(PythonOperator): upload_file_s3> on 2020-01-01T00:00:00+00:00
[2020-03-26 21:11:33,438] {standard_task_runner.py:53} INFO - Started process 7814 to run task
[2020-03-26 21:11:33,591] {logging_mixin.py:112} INFO - Running %s on host %s <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [running]> 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2020-03-26 21:11:33,776] {logging_mixin.py:112} INFO - [2020-03-26 21:11:33,776] {credentials.py:1196} INFO - Found credentials in shared credentials file: ~/.aws/credentials
[2020-03-26 21:11:43,023] {logging_mixin.py:112} INFO - An error occurred (400) when calling the HeadObject operation: Bad Request
[2020-03-26 21:11:43,024] {logging_mixin.py:112} INFO - Object exists
[2020-03-26 21:11:43,028] {python_operator.py:114} INFO - Done. Returned value was: None
[2020-03-26 21:11:43,043] {taskinstance.py:1048} INFO - Marking task as SUCCESS.dag_id=upload_s3, task_id=upload_file_s3, execution_date=20200101T000000, start_date=20200326T111133, end_date=20200326T111143
[2020-03-26 21:11:43,338] {logging_mixin.py:112} INFO - [2020-03-26 21:11:43,337] {local_task_job.py:103} INFO - Task exited with return code 0
[2020-03-26 21:23:45,873] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 21:23:45,901] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 21:23:45,901] {taskinstance.py:866} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 21:23:45,901] {taskinstance.py:867} INFO - Starting attempt 1 of 1
[2020-03-26 21:23:45,902] {taskinstance.py:868} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 21:23:45,926] {taskinstance.py:887} INFO - Executing <Task(PythonOperator): upload_file_s3> on 2020-01-01T00:00:00+00:00
[2020-03-26 21:23:45,934] {standard_task_runner.py:53} INFO - Started process 14450 to run task
[2020-03-26 21:23:46,106] {logging_mixin.py:112} INFO - Running %s on host %s <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [running]> 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2020-03-26 21:23:46,272] {logging_mixin.py:112} INFO - [2020-03-26 21:23:46,271] {credentials.py:1196} INFO - Found credentials in shared credentials file: ~/.aws/credentials
[2020-03-26 21:25:32,280] {logging_mixin.py:112} INFO - An error occurred (400) when calling the HeadObject operation: Bad Request
[2020-03-26 21:25:32,282] {logging_mixin.py:112} INFO - Object exists
[2020-03-26 21:25:32,287] {python_operator.py:114} INFO - Done. Returned value was: None
[2020-03-26 21:25:32,316] {taskinstance.py:1048} INFO - Marking task as SUCCESS.dag_id=upload_s3, task_id=upload_file_s3, execution_date=20200101T000000, start_date=20200326T112345, end_date=20200326T112532
[2020-03-26 21:25:36,622] {logging_mixin.py:112} INFO - [2020-03-26 21:25:36,621] {local_task_job.py:103} INFO - Task exited with return code 0
[2020-03-26 22:50:43,586] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 22:50:43,610] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 22:50:43,610] {taskinstance.py:866} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 22:50:43,610] {taskinstance.py:867} INFO - Starting attempt 1 of 1
[2020-03-26 22:50:43,610] {taskinstance.py:868} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 22:50:43,626] {taskinstance.py:887} INFO - Executing <Task(PythonOperator): upload_file_s3> on 2020-01-01T00:00:00+00:00
[2020-03-26 22:50:43,628] {standard_task_runner.py:53} INFO - Started process 85149 to run task
[2020-03-26 22:50:43,729] {logging_mixin.py:112} INFO - Running %s on host %s <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [running]> 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2020-03-26 22:50:43,770] {taskinstance.py:1128} ERROR - 'task_instance'
Traceback (most recent call last):
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 966, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 113, in execute
    return_value = self.execute_callable()
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 118, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/Users/tranquanghuy/email_analytics_dashoard/airflow_dir/dags/upload_to_S3.py", line 145, in upload_file_S3
    list_email = kwargs['task_instance'].xcom_pull(task_ids='extract_mail')
KeyError: 'task_instance'
[2020-03-26 22:50:43,791] {taskinstance.py:1185} INFO - Marking task as FAILED.dag_id=upload_s3, task_id=upload_file_s3, execution_date=20200101T000000, start_date=20200326T125043, end_date=20200326T125043
[2020-03-26 22:50:53,558] {logging_mixin.py:112} INFO - [2020-03-26 22:50:53,557] {local_task_job.py:103} INFO - Task exited with return code 1
[2020-03-26 23:08:38,499] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 23:08:38,531] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 23:08:38,532] {taskinstance.py:866} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 23:08:38,532] {taskinstance.py:867} INFO - Starting attempt 1 of 1
[2020-03-26 23:08:38,532] {taskinstance.py:868} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 23:08:38,541] {taskinstance.py:887} INFO - Executing <Task(PythonOperator): upload_file_s3> on 2020-01-01T00:00:00+00:00
[2020-03-26 23:08:38,543] {standard_task_runner.py:53} INFO - Started process 95264 to run task
[2020-03-26 23:08:38,655] {logging_mixin.py:112} INFO - Running %s on host %s <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [running]> 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2020-03-26 23:08:38,693] {taskinstance.py:1128} ERROR - 'task_instance'
Traceback (most recent call last):
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 966, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 113, in execute
    return_value = self.execute_callable()
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 118, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/Users/tranquanghuy/email_analytics_dashoard/airflow_dir/dags/upload_to_S3.py", line 158, in upload_file_S3
    list_email = kwargs['task_instance'].xcom_pull(task_ids='extract_mail')
KeyError: 'task_instance'
[2020-03-26 23:08:38,704] {taskinstance.py:1185} INFO - Marking task as FAILED.dag_id=upload_s3, task_id=upload_file_s3, execution_date=20200101T000000, start_date=20200326T130838, end_date=20200326T130838
[2020-03-26 23:08:48,483] {logging_mixin.py:112} INFO - [2020-03-26 23:08:48,482] {local_task_job.py:103} INFO - Task exited with return code 1
[2020-03-26 23:33:03,198] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 23:33:03,245] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 23:33:03,246] {taskinstance.py:866} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 23:33:03,246] {taskinstance.py:867} INFO - Starting attempt 1 of 1
[2020-03-26 23:33:03,246] {taskinstance.py:868} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 23:33:03,266] {taskinstance.py:887} INFO - Executing <Task(PythonOperator): upload_file_s3> on 2020-01-01T00:00:00+00:00
[2020-03-26 23:33:03,269] {standard_task_runner.py:53} INFO - Started process 8124 to run task
[2020-03-26 23:33:03,435] {logging_mixin.py:112} INFO - Running %s on host %s <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [running]> 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2020-03-26 23:33:03,492] {taskinstance.py:1128} ERROR - 'ti'
Traceback (most recent call last):
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 966, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 113, in execute
    return_value = self.execute_callable()
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 118, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/Users/tranquanghuy/email_analytics_dashoard/airflow_dir/dags/upload_to_S3.py", line 158, in upload_file_S3
    list_email = kwargs['ti'].xcom_pull(task_ids='extract_mail')
KeyError: 'ti'
[2020-03-26 23:33:03,503] {taskinstance.py:1185} INFO - Marking task as FAILED.dag_id=upload_s3, task_id=upload_file_s3, execution_date=20200101T000000, start_date=20200326T133303, end_date=20200326T133303
[2020-03-26 23:33:13,138] {logging_mixin.py:112} INFO - [2020-03-26 23:33:13,137] {local_task_job.py:103} INFO - Task exited with return code 1
[2020-03-26 23:37:27,231] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 23:37:27,269] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 23:37:27,270] {taskinstance.py:866} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 23:37:27,270] {taskinstance.py:867} INFO - Starting attempt 1 of 1
[2020-03-26 23:37:27,270] {taskinstance.py:868} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 23:37:27,289] {taskinstance.py:887} INFO - Executing <Task(PythonOperator): upload_file_s3> on 2020-01-01T00:00:00+00:00
[2020-03-26 23:37:27,292] {standard_task_runner.py:53} INFO - Started process 10399 to run task
[2020-03-26 23:37:27,439] {logging_mixin.py:112} INFO - Running %s on host %s <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [running]> 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2020-03-26 23:37:27,624] {logging_mixin.py:112} INFO - [2020-03-26 23:37:27,623] {credentials.py:1196} INFO - Found credentials in shared credentials file: ~/.aws/credentials
[2020-03-26 23:37:29,407] {taskinstance.py:1128} ERROR - 'S3' object has no attribute 'Object'
Traceback (most recent call last):
  File "/Users/tranquanghuy/email_analytics_dashoard/airflow_dir/dags/upload_to_S3.py", line 163, in upload_file_S3
    s3.Object(bucket, key).load()
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/boto3/resources/factory.py", line 505, in do_action
    response = action(self, *args, **kwargs)
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/boto3/resources/action.py", line 83, in __call__
    response = getattr(parent.meta.client, operation_name)(**params)
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/botocore/client.py", line 316, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/botocore/client.py", line 626, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (404) when calling the HeadObject operation: Not Found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 966, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 113, in execute
    return_value = self.execute_callable()
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 118, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/Users/tranquanghuy/email_analytics_dashoard/airflow_dir/dags/upload_to_S3.py", line 167, in upload_file_S3
    s3Object = s3.Object(bucket, key)
  File "/Users/tranquanghuy/email_analytics_dashoard/email_env/lib/python3.6/site-packages/botocore/client.py", line 566, in __getattr__
    self.__class__.__name__, item)
AttributeError: 'S3' object has no attribute 'Object'
[2020-03-26 23:37:29,429] {taskinstance.py:1185} INFO - Marking task as FAILED.dag_id=upload_s3, task_id=upload_file_s3, execution_date=20200101T000000, start_date=20200326T133727, end_date=20200326T133729
[2020-03-26 23:37:37,202] {logging_mixin.py:112} INFO - [2020-03-26 23:37:37,201] {local_task_job.py:103} INFO - Task exited with return code 1
[2020-03-26 23:40:03,642] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 23:40:03,667] {taskinstance.py:655} INFO - Dependencies all met for <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [queued]>
[2020-03-26 23:40:03,668] {taskinstance.py:866} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 23:40:03,668] {taskinstance.py:867} INFO - Starting attempt 1 of 1
[2020-03-26 23:40:03,668] {taskinstance.py:868} INFO - 
--------------------------------------------------------------------------------
[2020-03-26 23:40:03,685] {taskinstance.py:887} INFO - Executing <Task(PythonOperator): upload_file_s3> on 2020-01-01T00:00:00+00:00
[2020-03-26 23:40:03,687] {standard_task_runner.py:53} INFO - Started process 11901 to run task
[2020-03-26 23:40:03,795] {logging_mixin.py:112} INFO - Running %s on host %s <TaskInstance: upload_s3.upload_file_s3 2020-01-01T00:00:00+00:00 [running]> 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2020-03-26 23:40:03,903] {logging_mixin.py:112} INFO - [2020-03-26 23:40:03,902] {credentials.py:1196} INFO - Found credentials in shared credentials file: ~/.aws/credentials
[2020-03-26 23:40:10,745] {logging_mixin.py:112} INFO - Object exists
[2020-03-26 23:40:10,750] {python_operator.py:114} INFO - Done. Returned value was: None
[2020-03-26 23:40:10,766] {taskinstance.py:1048} INFO - Marking task as SUCCESS.dag_id=upload_s3, task_id=upload_file_s3, execution_date=20200101T000000, start_date=20200326T134003, end_date=20200326T134010
[2020-03-26 23:40:13,621] {logging_mixin.py:112} INFO - [2020-03-26 23:40:13,618] {local_task_job.py:103} INFO - Task exited with return code 0
